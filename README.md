# Natural Language Processing
This is the Natural Language Processing repository of IvLabs and contains implementation of various architectures, starting from "Character Level RNN(s)" built from scratch, up to and including the almighty "Transformer" architecture.
Further, we have also included a rough roadmap for enthusiasts with basic knowledge of Machine/Deep Learning.

We have implemented and compared the following architectures:
- [x] [Character Level RNN (Char RNN)](char_rnns)
    - [x] From scratch
    - [x] Vanilla RNN
    - [x] LSTM
    - [x] GRU

- [x] [Language Models (Word RNN)](word_rnn)
    - [x] Vanilla RNN
    - [x] LSTM
    - [x] GRU

- [x] [Neural Machine Translation](neural_machine_translation)\
    For Neural Machine Translation, we have implemented the following papers.
    - [x] [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
    - [x] [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
    - [x] [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)
    - [x] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

*The pending tasks and resources for studying and understading the required concepts will be added soon.*

<br />

### Contributors:
* [Rishika Bhagwatkar](https://https//github.com/rishika2110)
* [Khurshed P. Fitter](https://https//github.com/GlazeDonuts)
* [Aneesh A. Shetye](https://https//github.com/aneesh-shetye)
* [Diksha Bagade](https://github.com/Diksha942)
* [Kshitij Ambilduke](https://github.com/Kshitij-Ambilduke)
